import json
import os
import urllib, cStringIO
import urllib2
from collections import Counter
from datetime import datetime,date,timedelta
import copy

from PIL import Image as Im
import mysql.connector
#pip install -e git+https://github.com/tumblr/pytumblr.git#egg=pytumblr
import pytumblr
import networkx as nx
import pandas as pd
#pip install -e git+https://github.com/99designs/colorific#egg=colorific
import colorific
from matplotlib import colors
from dateutil.parser import parse
import hashlib
from pymarkovchain import MarkovChain
import string
import re

from images2gif import writeGif
from PIL import Image
import random


class tumblr_extract_controller(object):

    def __init__(self):
        secrets_file = open('secrets.json','rb')
        secrets = json.load(secrets_file)
        secrets_file.close()

        self.blog_name = "wheredidmypostgo"

        # Build an Authorized Tumblr Client
        self.tumblr_client = pytumblr.TumblrRestClient(**secrets['tumblr_tokens'])
        
        self.user_name = self.tumblr_client.info()['user']['name']

    def pull_tagged_tumblr_posts(self,tag_name,number_of_posts):
        '''
        returns a pandas data frame containing posts from tumblr
        '''
        import pandas as pd
        from datetime import date,time,datetime,timedelta

        before = datetime.today()

        data = self.tumblr_client.tagged(tag_name, before = before.strftime('%s'))

        list_of_posts = data
        for offset in range(20,number_of_posts,20):
            data = self.tumblr_client.tagged(tag_name, before = list_of_posts[-1]['timestamp'])
            try:
                list_of_posts.extend(data)
            except:
                pass

        posts_df = pd.DataFrame(list_of_posts)
        posts_df.timestamp = posts_df.timestamp.apply(datetime.fromtimestamp)

        self.posts_df = posts_df


    def pull_tumblr_post_by_id(self,user_name,post_id):
        '''
        returns a pandas data frame containing a particular tumblr post
        '''
        import pandas as pd
        from datetime import date,time,datetime
        data = self.tumblr_client.posts(user_name, notes_info='true', id=post_id, reblog_info='true')
        
        try:
            list_of_posts = data['posts']
                        
        except:
            list_of_posts = []

        try:
            posts_df = pd.DataFrame(list_of_posts)
            posts_df.timestamp = posts_df.timestamp.apply(datetime.fromtimestamp)
            self.blog_info = data['blog']
        except:
            pass
        
        self.posts = list_of_posts
        self.posts_df = posts_df


    def pull_tumblr_posts(self,user_name,number_of_posts):
        '''
        returns a pandas data frame containing posts from a particular tumblr blog
        '''
        import pandas as pd
        from datetime import date,time,datetime
        data = self.tumblr_client.posts(user_name, notes_info='true', reblog_info='true')
        self.blog_info = data['blog']
        total_posts = data['total_posts']
        list_of_posts = data['posts']
        for offset in range(20,min(number_of_posts,total_posts),20):
            data = self.tumblr_client.posts(user_name, notes_info='true', reblog_info='true', offset=offset)
            try:
                list_of_posts.extend(data['posts'])
            except:
                break

        posts_df = pd.DataFrame(list_of_posts)
        posts_df.timestamp = posts_df.timestamp.apply(datetime.fromtimestamp)
        self.posts = list_of_posts
        self.posts_df = posts_df


    def get_clean_tumblr_posttaglevel_data(self):
        '''
        takes a pandas data frame generated by the function pull_tumblr_posts() and returns
        a list of tuples posttag-level data
        '''
        rows = []
        for i,r in self.posts_df[['id','tags']].iterrows():    
            for tag in r['tags']:
                rows.append((r['id'],tag))
        return rows

    def get_clean_tumblr_notes(self):
        '''
        This function takes a pandas data frame generated by the function pull_tumblr_posts()
        and returns a pandas data frame containing note-level data.
        '''
        import pandas as pd
        from datetime import date,time,datetime
        notes = pd.DataFrame()
        posts_data_frame = self.posts_df
        for i in posts_data_frame.index:
            notes_list = posts_data_frame.notes[i]
            if isinstance(notes_list,list):
                post_notes = pd.DataFrame(notes_list)
                post_notes['orig_post_id'] = posts_data_frame.id[i]
                post_notes['orig_post_note count'] = posts_data_frame.note_count[i]
                post_notes['orig_post_link'] = posts_data_frame.post_url[i]
                post_notes['orig_post_timestamp'] = posts_data_frame.timestamp[i]
            else:
                post_notes = pd.DataFrame()
            if (i == 0):
                notes = post_notes
            else:
                notes = notes.append(post_notes)   
        notes.timestamp = notes.timestamp.apply(float).apply(datetime.fromtimestamp)
        return notes


class etl_controller(object):
    """ETL methods for collecting and storing reblog graphs"""
    def __init__(self):
        # Auth credentials are stored in secrets.json
        secrets_file = open('secrets.json','rb')
        secrets = json.load(secrets_file)
        secrets_file.close()

        self.blog_name = "wheredidmypostgo"

        # Build an Authorized Tumblr Client
        self.tumblr_client = pytumblr.TumblrRestClient(**secrets['tumblr_tokens'])

        # Build an Authorized Database Connection
        self.mysql_connection = mysql.connector.connect(use_unicode=True,
                                                        autocommit=True,
                                                        **secrets['mysql'])

        self.tb_extract_controller = tumblr_extract_controller()

        self.extract_target_metadata()


    def extract_target_metadata(self):

        curs = self.mysql_connection.cursor()

        sql = """
        select blog_name from wdmpg_targets
        where type = 'BLOG'
        """
        curs.execute(sql)
        self.target_blogs = [b[0] for b in curs.fetchall()]

        sql = """
        select blog_name, value from wdmpg_targets
        where type = 'POST'
        """
        curs.execute(sql)
        self.target_posts = [{'blog_name': p[0],'post_id': p[1]} for p in curs.fetchall()]

        sql = """
        select value from wdmpg_targets
        where type = 'TAG'
        """
        curs.execute(sql)
        self.target_tags = [t[0] for t in curs.fetchall()]

        curs.close()

    def check_submissions(self):

        def __generate_submissions(self):
            offset = 0
            batch =  self.tumblr_client.submission('wheredidmypostgo', offset = offset)['posts']
            yield batch
            while len(batch) == 10:
                offset += 10
                batch =  self.tumblr_client.submission('wheredidmypostgo', offset = offset)['posts']
                yield batch

        generator = __generate_submissions(self)
        submissions = []
        
        for p in generator:
            submissions.extend(p)
        
        keys = ['id','date','type','url','title','description','blog_name','liked','followed','post_url','reblog_key']
        columns = ",".join(keys)
        values = "%(" +")s,%(".join(keys) + ")s"
        updates = "id=id"
        
        sql = """
        INSERT INTO wdmpg_submissions (%s) 
        VALUES (%s)
        ON DUPLICATE KEY UPDATE %s
        """ % (columns,values,updates)
        
        curs = self.mysql_connection.cursor()
        if len(submissions)>1:
            for submission in submissions:
                if submission['type'] == 'link':
                    curs.execute(sql,{k:v for k,v in submission.iteritems() if k in keys})
                else:
                    pass
            print "multi"
        elif len(submissions)==1:
            if submissions[0]['type'] == 'link':
                curs.execute(sql,{k:v for k,v in submissions[0].iteritems() if k in keys})
                print "one"
            else:
                pass
        else:
            print "none"
            pass
        curs.close()

        # Add pending submissions to targets

        sql = """
        select * from wdmpg_submissions
        where response_generated = 0
        """
        curs = self.mysql_connection.cursor()
        curs.execute(sql)
        pending_submissions = [{k:v[i] for i,k in enumerate(curs.column_names)} for v in curs.fetchall()]
        curs.close()

        for s in pending_submissions:
            ## Test whether this submission is still pending
            # Retrieve Submission from API
            response = self.tumblr_client.posts('wheredidmypostgo', id = s['id'])

            if 'meta' in response.keys():

                print "No such submission.  Assuming response generated and updating wdmpg_submissions and wdmpg_targets."
                print "    submission_id = ", s['id']
                print "    post_url = ", s['url']

                sql = '''
                UPDATE wdmpg_submissions 
                SET response_generated = 1 
                WHERE id = %s
                ''' % s['id']
                curs = self.mysql_connection.cursor()
                curs.execute(sql)
                curs.close()

                if '.tumblr.com/post/' in s['url']:
                    sql = """
                    DELETE FROM wdmpg_targets 
                    WHERE TYPE = 'POST'
                    AND value = %s
                    """ % s['url'].split('/')[4]
                    curs = self.mysql_connection.cursor()
                    curs.execute(sql)
                    curs.close()

            else:
                ## Test whether this is a valid submission
                if s['type'] == 'link':
                    if '.tumblr.com/post/' in s['url']:
                        try:
                            blog_name = s['url'].split('/')[2].split('.')[0]
                            post_id = s['url'].split('/')[4]
                            sql = """
                            INSERT INTO wdmpg_targets (type,blog_name,value) 
                            VALUES ('POST', %s, %s)
                            ON DUPLICATE KEY UPDATE value = value
                            """ 
                            print sql%(blog_name,post_id)
                            curs = self.mysql_connection.cursor()
                            curs.execute(sql,(blog_name,post_id))
                            curs.close()
                        
                            print "Tumblr post ETL for "+blog_name+" post id "+str(post_id)
                            print "     extract . . . "
                            self.tb_extract_controller.pull_tumblr_post_by_id(blog_name,post_id)
                            print "     transform . . . "
                            transformed_df = self.transform_tb_posts()
                            print "     load posts . . . "
                            self.load_tb_posts(transformed_df)
                            print "     load notes . . . "
                            self.transform_and_load_tb_notes()
                            print "     inspect reblog tree . . . "
                            self.inspect_tb_reblog_tree(blog_name, post_id)
                        except Exception, e:
                            pass 
                    else:
                        pass
                else:
                    pass


    def tb_posts_etl(self):
        blog_names = self.target_blogs
        pd.np.random.shuffle(blog_names)
        for blog_name in blog_names:

            try:
                print "tb posts etl for ",blog_name

                # test new blog inspection method
                self.inspect_tb_blog_info(blog_name)
                print "   loaded blog info."
                print "   extracting posts . . ."

                # how many posts should we extract?
                curs = self.mysql_connection.cursor(buffered=True)
                sql = "select posts from tb_blogs where name = '%s' limit 1" % blog_name
                curs.execute(sql)
                try:
                    current_posts = int(curs.fetchall()[0][0])
                except:
                    current_posts = 0
                curs.close()


                # This convoluted logic is here to so that for blogs like americanapparel
                # which currently has ~15,000 posts we won't get stuck and block the rest of the ETL
                if current_posts > 500:
                    posts_to_extract = 500
                else:
                    posts_to_extract = current_posts + 100

                # test new post loading method
                self.tb_extract_controller.pull_tumblr_posts(blog_name, posts_to_extract)
                print "   posts extracted."

                transformed_df = self.transform_tb_posts()
                print "   posts transformed."
                print "   loading posts, photos and tags . . ."
                self.load_tb_posts(transformed_df)
                print "   posts, photos and tags loaded."

                print "   loading notes . . ."
                self.transform_and_load_tb_notes()
                print "   notes loaded."

                sql = """
                select id from tb_posts
                where blog_name = %s
                order by id DESC
                limit 40
                """
                curs = self.mysql_connection.cursor()
                curs.execute(sql, (blog_name,))
                post_ids = [i[0] for i in curs]
                curs.close()

                for post_id in post_ids:
                    print "     inspecting reblog tree for blog "+blog_name+" post id "+str(post_id)+" . . . "
                    self.inspect_tb_reblog_tree(blog_name, post_id)
            except Exception, e:
                pass
            
            

        tag_targets = self.target_tags
        pd.np.random.shuffle(tag_targets)
        for tag in tag_targets:
            try:
                print "Tumblr tags ETL for "+tag
                print "     extract . . . "
                self.tb_extract_controller.pull_tagged_tumblr_posts(tag, 500)
                print "     transform . . . "
                transformed_df = self.transform_tb_posts()
                print "     load posts . . . "
                self.load_tb_posts(transformed_df)
                print "     load notes . . . "
                self.transform_and_load_tb_notes()
            except Exception, e:
                pass
            

        post_targets = self.target_posts
        pd.np.random.shuffle(post_targets)
        for post in post_targets:
            try:
                print "Tumblr posts ETL for "+post['blog_name']+" post id "+post['post_id']
                print "     extract . . . "
                self.tb_extract_controller.pull_tumblr_post_by_id(post['blog_name'],post['post_id'])
                print "     transform . . . "
                transformed_df = self.transform_tb_posts()
                print "     load posts . . . "
                self.load_tb_posts(transformed_df)
                print "     load notes . . . "
                self.transform_and_load_tb_notes()
            except Exception, e:
                pass

    def etl_target_posts(self):
        
        post_targets = self.target_posts
        pd.np.random.shuffle(post_targets)
        
        for post in post_targets:
            try:
                print "Tumblr posts ETL for "+post['blog_name']+" post id "+post['post_id']
                print "     extract . . . "
                self.tb_extract_controller.pull_tumblr_post_by_id(post['blog_name'],post['post_id'])
                print "     transform . . . "
                transformed_df = self.transform_tb_posts()
                print "     load posts . . . "
                self.load_tb_posts(transformed_df)
                print "     load notes . . . "
                self.transform_and_load_tb_notes()
                print "     inspecting reblog tree . . . "
                self.inspect_tb_reblog_tree(post['blog_name'], post['post_id'])
            except Exception, e:
                pass


    def transform_tb_posts(self):
        df = self.tb_extract_controller.posts_df.copy()
        df['date'] = df.date.apply(parse)
        try:
            df['featured_in_tag'] = df.featured_in_tag.apply(lambda x: ", ".join(x) if isinstance(x,list) else None)
        except Exception, e:
            df['featured_in_tag'] = None
        df['highlighted'] = df.highlighted.apply(lambda x: ", ".join(x) if isinstance(x,list) else None)
        df['tags'] = df.tags.apply(lambda x: ", ".join(x) if isinstance(x,list) else None)

        fields = ['id','blog_name','date','featured_in_tag','liked','followed','note_count','post_url','reblog_key','reblogged_from_id','reblogged_from_name','reblogged_from_url','reblogged_from_title','reblogged_root_title','reblogged_root_name','reblogged_root_url','source','short_url','type']

        for column in df.columns:
            if column not in fields:
                try:
                    del df[column]
                except:
                    pass
        return df

    def extract_tb_posts_table(self, blog_name):
        """
        extracts the tb_post_level table for blog_name 
        """
        sql = "select * from tb_posts where blog_name = '%s'" % blog_name
        df = psql.read_frame(sql,self.mysql_connection)
        return df

    def transform_and_load_tb_posttags(self):
        """
        Takes data
        """
        tag_data = self.tb_extract_controller.get_clean_tumblr_posttaglevel_data()
        curs = self.mysql_connection.cursor(buffered=True)
        query = 'INSERT INTO tb_posttag_level (post_id,tag,id) VALUES (%s,%s,%s) ON DUPLICATE KEY UPDATE tag = tag'
        hashed_data = [(t[0], t[1], hashlib.md5(str(t[0])+unicode(t[1]).encode('ascii','ignore')).digest()) for t in tag_data]
        chunks = [hashed_data[x:x+100] for x in xrange(0,len(hashed_data),100)]
        # execute these in chunks
        for chunk in chunks:
            curs.executemany(query,chunk)
        curs.close()

    def load_tb_posts(self, transformed_df):
        """
        backfills the tb_posts table with data from transform_tb_posts()
        """
        curs = self.mysql_connection.cursor(buffered=True)

        for i,p in transformed_df.iterrows():
            post_dict = dict(p)


            for k,v in post_dict.items():
                if pd.isnull(post_dict[k]):
                    post_dict[k] = None

            columns = ', '.join(post_dict.keys())
            placeholders = "%("+')s,%('.join(post_dict.keys()) + ")s"
            updates = "note_count = %(note_count)s"
            
            def nan_to_none(v):
                if pd.isnull(v):
                    return None
                else:
                    return v
                
            for k,v in post_dict.iteritems():
                post_dict[k] = nan_to_none(v)

            
                query = """
                INSERT INTO tb_posts (%s) VALUES (%s)
                ON DUPLICATE KEY UPDATE %s
                """ % (columns, placeholders, updates)

                curs.execute(query, post_dict)
        
        
        self.transform_and_load_tb_posttags()
        
        try:
            self.transform_and_load_tb_photos()
        except:
            pass

        curs.close()


    def load_tb_blog_info(self):
        """
        loads the blog info of the most recently pulled post data
        """
        blog_info = self.tb_extract_controller.blog_info
        cols = [u'ask',u'info_inspected',u'info_last_inspected',u'ask_anon',u'can_send_fan_mail',u'facebook',u'facebook_opengraph_enabled',u'tweet',u'twitter_enabled',u'twitter_send',u'updated',u'description',u'title',u'url',u'share_likes',u'posts',u'is_nsfw'u'likes',u'name']
        blog_info['info_inspected'] = True
        blog_info['info_last_inspected'] = datetime.today()
        try:
            blog_info['updated'] = datetime.fromtimestamp(float(blog_info['updated']))
        except:
            blog_info['updated'] = None
        columns = ','.join(list(set(cols) & set(blog_info.keys())))
        placeholders = "%("+')s,%('.join(list(set(cols) & set(blog_info.keys()))) + ")s"
        updates = ",".join([k+"=%("+k+")s" for k in list(set(cols) & set(blog_info.keys()))])

        curs = self.mysql_connection.cursor(buffered=True)
        try:
            sql = """
            INSERT into tb_blogs (%s) VALUES (%s)
            ON DUPLICATE KEY UPDATE %s
            """ % (columns, placeholders, updates)
            curs.execute(sql,blog_info)
        except:
            pass
        curs.close()

    def inspect_tb_blog_info(self,blog_name):
        blog_info = self.tumblr_client.blog_info(blog_name)['blog']
        cols = [u'ask',u'info_inspected',u'info_last_inspected',u'ask_anon',u'can_send_fan_mail',u'facebook',u'facebook_opengraph_enabled',u'tweet',u'twitter_enabled',u'twitter_send',u'updated',u'description',u'title',u'url',u'share_likes',u'posts',u'is_nsfw'u'likes',u'name']
        blog_info['info_inspected'] = True
        blog_info['info_last_inspected'] = datetime.today()
        blog_info['updated'] = datetime.fromtimestamp(blog_info['updated'])
        columns = ','.join(list(set(cols) & set(blog_info.keys())))
        placeholders = "%("+')s,%('.join(list(set(cols) & set(blog_info.keys()))) + ")s"
        updates =  ",".join([k+"=%("+k+")s" for k in list(set(cols) & set(blog_info.keys()))])

        curs = self.mysql_connection.cursor(buffered=True)

        sql = """
        INSERT into tb_blogs (%s) VALUES (%s)
        ON DUPLICATE KEY UPDATE %s
        """ % (columns, placeholders,updates)
        curs.execute(sql,blog_info)

        curs.close()

    def tb_reblog_tree_etl_active_posts(self):

        # fast posts etl to collect new posts
        blog_targets = self.target_blogs
        pd.np.random.shuffle(blog_targets)
        for blog_name in blog_targets:
            try:
                print "Fast Tumblr posts ETL for "+blog_name
                print "     extract . . . "
                self.tb_extract_controller.pull_tumblr_posts(blog_name, 20)
                print "     transform . . . "
                transformed_df = self.transform_tb_posts()
                print "     load posts . . . "
                self.load_tb_posts(transformed_df)
                print "     load notes . . . "
                self.transform_and_load_tb_notes()
            except Exception, e:
                pass
            

        tag_targets = self.target_tags
        pd.np.random.shuffle(tag_targets)
        for tag in tag_targets:
            try:
                print "Fast Tumblr tags ETL for "+tag
                print "     extract . . . "
                self.tb_extract_controller.pull_tagged_tumblr_posts(tag, 20)
                print "     transform . . . "
                transformed_df = self.transform_tb_posts()
                print "     load posts . . . "
                self.load_tb_posts(transformed_df)
                print "     load notes . . . "
                self.transform_and_load_tb_notes()
            except Exception, e:
                pass
            

        post_targets = self.target_posts
        pd.np.random.shuffle(post_targets)
        for post in post_targets:
            try:
                print "Fast Tumblr posts ETL for "+post['blog_name']+" post id "+post['post_id']
                print "     extract . . . "
                self.tb_extract_controller.pull_tumblr_post_by_id(post['blog_name'],post['post_id'])
                print "     transform . . . "
                transformed_df = self.transform_tb_posts()
                print "     load posts . . . "
                self.load_tb_posts(transformed_df)
                print "     load notes . . . "
                self.transform_and_load_tb_notes()
            except Exception, e:
                pass
            

        curs = self.mysql_connection.cursor(buffered=True)
        sql = """
        select blog_name, id from tb_posts 
        where notes_per_day > 15
            and reblogged_from_id is NULL 
        order by RAND()
        """
        curs.execute(sql)
        blogs_and_posts = [i for i in curs.fetchall()]
        curs.close()
        for blog_name,post_id in blogs_and_posts:
            try:
                print "Inspecting "+blog_name+" post id "+post_id
                print "     extract . . . "
                self.tb_extract_controller.pull_tumblr_post_by_id(blog_name,post_id)
                print "     transform . . . "
                transformed_df = self.transform_tb_posts()
                print "     load posts . . . "
                self.load_tb_posts(transformed_df)
                print "     load notes . . . "
                self.transform_and_load_tb_notes()
                print "     inspect reblog tree . . . "
                self.inspect_tb_reblog_tree(blog_name, post_id)
            except Exception, e:
                pass
            


    def tb_reblog_tree_etl_targets(self):

        sql = """
        select distinct date, blog_name, tb_posts.id, reblogs_last_crawled, notes_per_day, note_count
        from tb_posts 
        join tb_posttag_level on tb_posttag_level.post_id = tb_posts.id
        where blog_name in (%s)
            OR tag in (%s)
            OR tb_posts.id in (%s)
        and reblogged_from_id is NULL
        """ % ("'"+"', '".join(self.target_blogs)+"'",
               "'"+"', '".join(self.target_tags)+"'",
               "'"+"', '".join([p['post_id'] for p in self.target_posts])+"'")
        print sql

        curs = self.mysql_connection.cursor(buffered=True)
        curs.execute(sql)

        def calculate_days_since_crawl(reblogs_last_crawled):
            if reblogs_last_crawled != None:
                return (datetime.now()-reblogs_last_crawled).total_seconds()/86400 
            elif (note_count == None):
                return 500 #Fake value to bump these up in the queue
            else:
                return (datetime.now()-date).total_seconds()/86400

        def calculate_notes_per_day(note_count,date,notes_per_day):
            if notes_per_day != None:
                return notes_per_day
            elif (note_count == None):
                return 500 #Fake value to bump these up in the queue
            else:
                return float(note_count) / ((datetime.now()-date).total_seconds()/86400)

        posts = [{'id': post_id,
                  'blog_name': blog_name,
                  'days_since_crawl': calculate_days_since_crawl(reblogs_last_crawled),
                  'notes_per_day': calculate_notes_per_day(note_count,date,notes_per_day)}
                for date, blog_name, post_id, reblogs_last_crawled, notes_per_day, note_count in curs]
        curs.close()


        from operator import itemgetter

        for blog_name,post_id,notes_per_day in sorted([(p['blog_name'],p['id'],p['notes_per_day'] * abs(p['days_since_crawl'])) for p in posts],key=itemgetter(2), reverse=True)[0:500]:
            try:
                print "Inspecting "+blog_name+" post id "+str(post_id)
                print "     extract . . . "
                self.tb_extract_controller.pull_tumblr_post_by_id(blog_name,post_id)
                print "     transform . . . "
                transformed_df = self.transform_tb_posts()
                print "     load post . . . "
                self.load_tb_posts(transformed_df)
                print "     load notes . . . "
                self.transform_and_load_tb_notes()
                print "     inspect reblog tree . . . "
                self.inspect_tb_reblog_tree(blog_name, post_id)
            except Exception, e:
                pass
            

    
    def inspect_tb_reblog_tree(self,blog_name, post_id):
        
        # put the freshest data in the database
        self.tb_extract_controller.pull_tumblr_post_by_id(blog_name,post_id)
        self.transform_and_load_tb_notes()
        
        # fetch the root url
        curs = self.mysql_connection.cursor(buffered=True)
        sql = "SELECT post_url from tb_posts WHERE id = '%s'"
        curs.execute(sql % post_id)

        root_url = str(curs.fetchall()[0][0])

        sql = "UPDATE tb_posts SET reblogs_last_crawled = %s WHERE id = %s"
        curs.execute(sql,(datetime.now(),post_id))
        curs.close()
        
        
        # select all known reblogs that are in tb_notes but not yet in tb_posts
        curs = self.mysql_connection.cursor(buffered=True)
        
        sql = """
        select blog_name,post_id
        from (SELECT blog_name,post_id FROM tb_notes
        where target_post_id = '%s' 
        and type = 'reblog') AS outer_expr
        WHERE outer_expr.post_id NOT IN (
            SELECT post_id from tb_posts
            INNER JOIN tb_notes on tb_notes.post_id = tb_posts.id
            WHERE post_id = outer_expr.post_id
            )
        ORDER BY RAND()
        """
        
        curs.execute(sql % post_id)
        
        new_reblogs = curs.fetchall()
        curs.close()
        print str(len(new_reblogs)) + " new reblogs in tb_notes"
        
        for blog_name,post_id in new_reblogs:
            try:
                # pull the post data
                self.tb_extract_controller.pull_tumblr_post_by_id(blog_name,post_id)
                
                # load the blog's info
                self.load_tb_blog_info()
                
                if self.tb_extract_controller.posts_df.shape[0] > 0 :
                    # Load the post along with its photos and tags
                    transformed_df = self.transform_tb_posts()
                    self.load_tb_posts(transformed_df)
                else:
                    pass
            except Exception, e:
                print str(e)
                break
        
        new_reblogs = ['1']

        iterations = 0
        reblogged_from_post_sets = []

        while len(new_reblogs) > 0 and iterations < 100: 
            # select all known reblogs that are in the reblogged_from fields of tb_posts
            # but not yet in tb_posts

            iterations += 1
            
            sql = """
            select distinct reblogged_from_name, reblogged_from_id  
            FROM tb_posts 
            WHERE reblogged_root_url = '%s'
            AND reblogged_from_id NOT IN (SELECT id from tb_posts)
            ORDER BY RAND()
            """ 
            curs = self.mysql_connection.cursor(buffered=True)
            curs.execute(sql % root_url)
            
            new_reblogs = curs.fetchall()
            curs.close()
            
            print str(len(new_reblogs)) + " new reblogs in the reblogged_from fields of tb_posts"
            

            reblogged_from_posts = set()

            for blog_name,post_id in new_reblogs:

                reblogged_from_posts.add(post_id) 

                try:
                    # pull the post data
                    self.tb_extract_controller.pull_tumblr_post_by_id(blog_name,post_id)
                    
                    # load the blog's info
                    self.load_tb_blog_info()
                    
                    if len(self.tb_extract_controller.posts) > 0:
                        # Load the post along with its photos and tags
                        transformed_df = self.transform_tb_posts()
                        self.load_tb_posts(transformed_df)
                    else:
                        pass
                except Exception, e:
                    print str(e)
                    new_reblogs = []
                    break

            reblogged_from_post_sets.append(reblogged_from_posts)

            if iterations >= 2:
                if reblogged_from_post_sets[-1] == reblogged_from_post_sets[-2]:
                    print "Post sets are identical between iterations.  Assuming posts are dead and breaking out of loop . . ."
                    break

    def transform_and_load_tb_notes(self):
        """
        takes current list of post objects from tb_extract_controller
        and loads notes to tb_notes and corresponding blogs to tb_blogs
        """
        posts = copy.deepcopy(self.tb_extract_controller.posts)
        notes = []

        def generate_note_hash(note):
            hash_string =  note['blog_name'] + note['type']
            if note['type'] == 'like':
                hash_string =  hash_string + str(note['timestamp']) 
            if note['type'] == 'reblog':
                hash_string =  hash_string + str(note['post_id'])  
            h = hashlib.md5(hash_string)
            return h.digest()
        
        for p in posts:
            if "notes" in p.keys():
                post_notes = p["notes"]
                for n in post_notes:
                    n['timestamp'] = datetime.fromtimestamp(float(n['timestamp'])) if type(n['timestamp']) == unicode or type(n['timestamp']) == int else n['timestamp']
                    n['target_post_id'] = p['id']
                    n['id'] = generate_note_hash(n)
                notes.extend(post_notes)
            else:
                pass
        

        curs = self.mysql_connection.cursor(buffered=True)

        likes = [n for n in notes if n['type'] == 'like']
        reblogs =  [n for n in notes if n['type'] == 'reblog']
        
        if len(likes)>0:
            columns = ",".join(likes[0].keys())
            placeholders = "%("+')s,%('.join(likes[0].keys()) + ")s"
            
            sql = """
            INSERT into tb_notes (%s) VALUES (%s)
            ON DUPLICATE KEY UPDATE timestamp=timestamp
            """  % (columns, placeholders)
            try:
                curs.executemany(sql,likes)
            except:
                for like in likes:
                    columns = ",".join(like.keys())
                    placeholders = "%("+')s,%('.join(like.keys()) + ")s"
                    
                    sql = """
                    INSERT into tb_notes (%s) VALUES (%s)
                    ON DUPLICATE KEY UPDATE timestamp=timestamp
                    """  % (columns, placeholders)
                    curs.execute(sql,like)

        
        if len(reblogs)>0:
            columns = ",".join(reblogs[0].keys())
            placeholders = "%("+')s,%('.join(reblogs[0].keys()) + ")s"
            
            sql = """
            INSERT into tb_notes (%s) VALUES (%s)
            ON DUPLICATE KEY UPDATE timestamp=timestamp
            """  % (columns, placeholders)
            
            try:
                curs.executemany(sql,reblogs) 
            except:
                for reblog in reblogs:
                    columns = ",".join(reblog.keys())
                    placeholders = "%("+')s,%('.join(reblog.keys()) + ")s"
                    
                    sql = """
                    INSERT into tb_notes (%s) VALUES (%s)
                    ON DUPLICATE KEY UPDATE timestamp=timestamp
                    """  % (columns, placeholders)
                    curs.execute(sql,reblog)

        # calculate notes per day heuristic
        
        posts_with_notes = [p for p in posts if 'notes' in p.keys()]
        posts_with_notes = [p for p in posts_with_notes if len(p['notes'])>1]
        note_timestamps = [[n['timestamp'] for n in p['notes']] for p in posts_with_notes]
        notes_per_day = [len(l) / ((max(l) - min(l)).total_seconds() / 86400) for l in note_timestamps]

        for i,npd in enumerate(notes_per_day):
           posts_with_notes[i]['notes_per_day'] = npd

        sql = "UPDATE tb_posts SET notes_last_inspected = %s, notes_per_day = %s where id = %s"
        curs.executemany(sql,[(datetime.now(),p['notes_per_day'],p['id']) for p in posts_with_notes])

        curs.close()


    def transform_and_load_tb_photos(self):
        
        def transform_photo_data(photo_data):
            if type(photo_data) == list:
                for photo in photo_data:
                    data = photo['original_size']
                    for alt_size in photo['alt_sizes']:
                        if alt_size['width'] in set([75,100,250,400,500,1000]):
                            key = str(alt_size['width']) + "w_url"
                            data[key] = alt_size['url']
                        else:
                            pass
                return data
            else:
                return None
        
        self.tb_extract_controller.posts_df.index = self.tb_extract_controller.posts_df.id

        curs = self.mysql_connection.cursor(buffered=True)

        if 'photos' in self.tb_extract_controller.posts_df.columns:

            s = self.tb_extract_controller.posts_df.photos.apply(transform_photo_data)
            
            for k,v in s.iterkv():
                if pd.isnull(k):
                    pass
                elif pd.isnull(v):
                    pass
                else:
                    columns = ', '.join(v.keys())
                    placeholders = "%("+')s,%('.join(v.keys()) + ")s"
                    sql = """
                    INSERT into tb_photos (%s,post_id) VALUES (%s,%s)
                    ON DUPLICATE KEY UPDATE url=url
                    """ % (columns, placeholders,k)
                    curs.execute(sql,v)

        else:
            # no photos in the data so we're cool
            pass

        curs.close()



    def extract_tumblr_reblog_graph(self,reblogged_root_name,end_date,lookback_days):
        """
        Returns a directed graph (networkx.DiGraph) of reblogs from a particular root blog
        nodes represent blogs, edges represent reblogged_from > reblogged_to relationships, 
        edge weights represent counts of reblogs along a directed edge
        
        Parameters
        ----------
        reblogged_root_name (str or unicode) the root blog to extract reblogs from
        end_date (datetime.date) the final date of the window 
        days (int) the number days to look back
        """
        
        start_date = end_date - timedelta(days=lookback_days)
        
        sql = """
        select blog_name,reblogged_from_name from tb_posts
        where reblogged_root_name = %s
        and date between %s and %s
        """
        
        curs = self.mysql_connection.cursor()
        curs.execute(sql, (reblogged_root_name, start_date.isoformat(),end_date.isoformat()))
        
        flow_graph = nx.DiGraph()
        
        for blog_name,reblogged_from_name in curs:
                
            if flow_graph.has_edge(reblogged_from_name,blog_name):
                flow_graph.edge[reblogged_from_name][blog_name]['weight'] += 1 
            else:
                flow_graph.add_edge(reblogged_from_name,blog_name,weight=1)
        
        curs.close()
        return flow_graph

    def calculate_tumblr_reblog_graph_metrics(self,flow_graph):
        """
        Returns a pandas data frame with three influence metrics for each node in the reblog graph G
        
        ClosenessCentrality at a node is 1/average distance to all other nodes.  The distance used in the shortest path 
            calculations for each edge is 1 / reblog_count. The implicit assumption is that nodes that have a short distance to other nodes can disseminate
            information more quickly on the network. See
            Freeman, L.C., 1979. Centrality in networks: I. conceptual clarifcation. Social Networks 1, 215-239.
            Borgatti, Stephen P., and Martin G. Everett. "A graph-theoretic perspective on centrality." Social networks 28, no. 4 (2006): 466-484.
        
        DirectReblogs is the count of direct child reblogs from a given node
        
        SuccessiveReblogs is the count of reblogs in the full BFS tree from a given node
        
        Parameters
        ----------
        flow_graph (networkx.DiGraph) a directed reblog graph generated by the function extract_tumblr_reblog_graph
        """
        
        df = pd.DataFrame(index=flow_graph.nodes())
        
        df['blog_name'] = df.index

        df['SuccessiveReblogs'] = pd.Series({node: sum([flow_graph.edge[a][b]['weight'] for a,b in nx.bfs_edges(flow_graph,node)]) 
                                             for node in flow_graph.nodes_iter()})
        df['DirectReblogs'] = pd.Series({node: len(flow_graph[node]) for node in flow_graph.nodes_iter()})

        #
        # Closeness Centrality Calculation
        #

        # Distances
        for f,t,d in flow_graph.edges_iter(data=True):
            d['inverse_weight'] = 1 / d['weight']
        
        # Algorithm
        df['ClosenessCentrality'] = pd.Series(nx.closeness_centrality(flow_graph,distance='inverse_weight',normalized=True))

        # Normalize
        df['ClosenessCentrality'] = df.ClosenessCentrality / df.ClosenessCentrality.sum()
        
        return df
    
    def etl_target_blog_reblog_graphs(self):
        for blog in self.target_blogs:
            self.etl_tumblr_reblog_graph(blog,date.today(),7)                    
        
    def etl_tumblr_reblog_graph(self,reblogged_root_name,end_date,lookback_days):
        """"
        Extracts a reblog graph for particular root blog and time period,
        calculates three influence metrics and loads the data to tb_reblog_graphs 
        
        The Reblog Graph
        A directed graph (networkx.DiGraph) of reblogs from a particular root blog
        nodes represent blogs, edges represent reblogged_from > reblogged_to relationships, 
        edge weights represent counts of reblogs along a directed edge

        The Metrics
        ClosenessCentrality at a node is 1/average distance to all other nodes.  The distance used in the shortest path 
            calculations for each edge is 1 / reblog_count. The implicit assumption is that nodes that have a short distance to other nodes can disseminate
            information more quickly on the network. See
            Freeman, L.C., 1979. Centrality in networks: I. conceptual clarifcation. Social Networks 1, 215-239.
            Borgatti, Stephen P., and Martin G. Everett. "A graph-theoretic perspective on centrality." Social networks 28, no. 4 (2006): 466-484.

        DirectReblogs the count of direct child reblogs from a given node

        SuccessiveReblogs the count of reblogs in the full BFS tree from a given node
        
        Parameters
        ----------
        reblogged_root_name (str or unicode) the root blog to extract reblogs from
        end_date (datetime.date) the final date of the window 
        days (int) the number days to look back
        """
        
        G = self.extract_tumblr_reblog_graph(reblogged_root_name,end_date,lookback_days)
        df = self.calculate_tumblr_reblog_graph_metrics(G)


        start_date = end_date - timedelta(days=lookback_days)
        cols = ['blog_name','ClosenessCentrality','SuccessiveReblogs','DirectReblogs']
        placeholders = ", ".join(["%("+col+")s" for col in cols])
        updates = ", ".join([col+" = %("+col+")s" for col in cols if col != 'blog_name'])
        
        sql = """
        INSERT INTO tb_reblog_graphs (reblogged_root_name,start_date,end_date,lookback_days,blog_name,ClosenessCentrality,SuccessiveReblogs,DirectReblogs)
        VALUES ('%s','%s','%s',%s,%s)
        ON DUPLICATE KEY UPDATE %s
        """ % (reblogged_root_name,start_date,end_date,lookback_days,placeholders,updates)
        
        metric_list = []
        
        for blog_name,row_data in df.iterrows():
            metrics = dict(row_data)
            m = dict()
            for k,v in metrics.items():
                m[k] = v
                if type(v) == pd.np.float64:
                    m[k] = float(v)
                if pd.isnull(v):
                    m[k] = None
            metric_list.append(m)

        curs = self.mysql_connection.cursor()
        for m in metric_list:
            curs.execute(sql,m)
        curs.close()


class gif_generator(object):
    """GIF graph generation methods"""
    def __init__(self):

        # Auth credentials are stored in secrets.json
        secrets_file = open('secrets.json','rb')
        secrets = json.load(secrets_file)
        secrets_file.close()

        # Build an Authorized Tumblr Client
        self.tumblr_client = pytumblr.TumblrRestClient(**secrets['tumblr_tokens'])

        # Build an Authorized Database Connection
        self.mysql_connection = mysql.connector.connect(**secrets['mysql'])

        # Attach an ETL controller
        self.etl_controller = etl_controller()

    def extract_reblog_graph(self, post_url, frame_rate_multiplier=1):
        
        opener = urllib2.build_opener(urllib2.HTTPRedirectHandler)
        request = opener.open(post_url)
        post_url = request.url

        post_id = post_url.split('/post/')[1].split('/')[0]

        G = nx.DiGraph()

        sql = """
        select blog_name, date, id
        from tb_posts
        where id = %s
        """
        curs = self.mysql_connection.cursor()
        curs.execute(sql,(post_id,))
        for node in curs:
            G.add_node(node[0])
            self.blog_name = node[0]
            self.post_date = node[1]
            self.post_id = node[2]
        curs.close()

        print "Tumblr post ETL for "+self.blog_name+" post id "+str(self.post_id)
        print "     extract . . . "
        self.etl_controller.tb_extract_controller.pull_tumblr_post_by_id(self.blog_name,self.post_id)
        print "     transform . . . "
        transformed_df = self.etl_controller.transform_tb_posts()
        print "     load posts . . . "
        self.etl_controller.load_tb_posts(transformed_df)
        print "     load notes . . . "
        self.etl_controller.transform_and_load_tb_notes()
        print "     inspect reblog tree . . . "
        self.etl_controller.inspect_tb_reblog_tree(self.blog_name, self.post_id)

        print "Building graph . . ."

        sql = """
        select reblogged_from_name, blog_name, date
        from tb_posts
        where reblogged_root_url = %s
        order by date ASC
        """

        curs = self.mysql_connection.cursor()
        curs.execute(sql,(post_url,))
        for edge in curs:
            G.add_edge(edge[0],edge[1])
            G.edge[edge[0]][edge[1]]['date'] = edge[2]
        curs.close()

        max_connected_components = random.randint(1,int(6 + -.0025 * G.number_of_nodes()))

        connected=nx.connected_component_subgraphs(G.to_undirected())[0:max_connected_components]

        self.connected_components = len(connected)

        self.G = G.subgraph(connected[0].nodes())

        for cc in connected[1:]:
            self.G = nx.compose(self.G,G.subgraph(cc.nodes()))

        

        for edge in self.G.edges_iter(data=True):
            edge[2]['timestamp'] = int(edge[2]['date'].strftime('%s'))
        
        print "Graph Built. Sequencing edges . . ."
        print "    node count = ", str(self.G.number_of_nodes())

        if self.G.number_of_nodes() < 80:
            frames = int(self.G.number_of_nodes() / 1.8 * frame_rate_multiplier)
        elif self.G.number_of_nodes() < 150:
            frames = int(45 * frame_rate_multiplier)
        elif self.G.number_of_nodes() < 900:
            frames = int(36 * frame_rate_multiplier)
        elif self.G.number_of_nodes() < 1800:
            frames = int(30 * frame_rate_multiplier)
        else:
            frames = int(24 * frame_rate_multiplier)

        print "    frame count = ", str(frames), "frame rate multiplier = ", frame_rate_multiplier

            
        timestamps = [int(data['date'].strftime('%s')) for fr,to,data in self.G.edges_iter(data=True)]

        # Cardinal Sequencing
        step = max(len(timestamps)/frames,1)

        self.edge_sequence = [sorted(self.G.edges(data=True), key=lambda e:e[2]['timestamp'])[0:x] for x in xrange(step,len(timestamps),step)]
        self.edge_sequence_dates = [max([d['date'] for f,t,d in edges]) for edges in self.edge_sequence]

        print "Edges Sequenced. Classifying Nodes by Centrality . . . "
        # Ordinal Sequencing Method
        #step = (max(timestamps) - min(timestamps))/frames
        #edge_sequence = [[(fr,to) for fr,to,data in G.edges_iter(data=True) if data['timestamp'] <= x+step] for x in xrange(min(timestamps),max(timestamps),step)]

        centrality = nx.closeness_centrality(self.G,normalized=True)
        centrality_class = pd.Series(centrality)

        threshold = 0.0001
        while threshold <= 1:
            try:
                centrality_class[centrality_class<threshold] = 1
                cat = pd.qcut(centrality_class[centrality_class!=1],
                              5,labels=[2,3,4,5,6])
                centrality_class[centrality_class!=1] = pd.np.array(cat)
                break
            except Exception, e:
                threshold += 0.0001
                continue


        self.centrality_class = centrality_class.to_dict()
        print "Nodes classified."


    def pick_colors(self, blog_name = None):
        if blog_name == None:
            response = self.tumblr_client.posts(self.blog_name, offset = random.randint(0,150))
        else:
            response = self.tumblr_client.posts(blog_name, offset = random.randint(0,150))

        thumbnails = [post['photos'][0]['alt_sizes'][-1] for post in response['posts'] if 'photos' in post.keys()]

        blog_page = urllib.urlopen(response['blog']['url'])
        background_colors = ["#"+line.strip().split("#")[1][0:6] for line in blog_page.readlines() if 'background: ' in line and "#" in line]
        background_colors = [color.lower() for color in background_colors if color.startswith('#') and len(color) == 7 and ' ' not in color]

        if len(background_colors) == 0:
            background_colors = ['#ffffff']

        most_common_background_color = Counter(background_colors).most_common(1)[0][0]
        most_common_background_color

        x=0
        y=0

        height = sum([t['height'] for t in thumbnails]) / (len(thumbnails) / 3)
        width = sum([t['width'] for t in thumbnails]) / (len(thumbnails) / 4)

        grid_image = Im.new("RGB", (width, height))

        for i,thumbnail in enumerate(thumbnails):

            thumbnail_file = cStringIO.StringIO(urllib.urlopen(thumbnail['url']).read())
            thumbnail_image = Im.open(thumbnail_file)
            
            grid_image.paste(thumbnail_image, (x,y))
            grid_image.save('grid.jpg')
            
            if (i+1) % 5 == 0:
                x = 0
                y += thumbnail['height']
            else:
                x += thumbnail['width']
                
        photo_pallete = colorific.extract_colors(Im.open(open('grid.jpg','rb')), 
                                                 min_saturation=0.15,
                                                 max_colors=7)
        colorific.save_palette_as_image('photo.png',photo_pallete)

        color_count = len(background_colors)
        color_counter = Counter(background_colors)

        try:
            page_pallete = colorific.Palette([colorific.Color(colorific.hex_to_rgb(color), 
                       prominence = float(count)/color_count) 
                       for color,count in color_counter.items()],None)
        except:
            page_pallete = colorific.Palette(colors=[colorific.Color(value=(255, 255, 255), prominence=1.0)], bgcolor=None)

        colorific.save_palette_as_image('page.png',page_pallete)


        self.sorted_photo_pallete = sorted(photo_pallete.colors, key=lambda c:colorific.colorsys.rgb_to_hsv(*colorific.norm_color(c.value))[1], reverse=True)
        self.sorted_page_pallete = sorted(page_pallete.colors, key=lambda c:colorific.colorsys.rgb_to_hsv(*colorific.norm_color(c.value))[1], reverse=False)

        from matplotlib import colors

        bg_i = random.randint(0,len(self.sorted_page_pallete)-1)
        self.background_color = colorific.norm_color(self.sorted_page_pallete[bg_i].value)

        # make a color map of fixed colors
        self.node_cmap = colors.ListedColormap([self.background_color,
                                      colorific.norm_color(self.sorted_photo_pallete[5].value),
                                      colorific.norm_color(self.sorted_photo_pallete[4].value),
                                      colorific.norm_color(self.sorted_photo_pallete[3].value),
                                      colorific.norm_color(self.sorted_photo_pallete[2].value),
                                      colorific.norm_color(self.sorted_photo_pallete[1].value),
                                      colorific.norm_color(self.sorted_photo_pallete[0].value)])
        bounds=[0,1,2,3,4,5,6]
        norm = colors.BoundaryNorm(bounds, self.node_cmap.N, clip=True)


        self.edge_cmap = colors.ListedColormap([self.background_color,
                                      colorific.norm_color(self.sorted_photo_pallete[6].value)])
        bounds=[0,1]
        norm = colors.BoundaryNorm(bounds, self.edge_cmap.N, clip=True)


    def draw_graph_frames(self):


        # use graphviz to find layout
        if self.G.number_of_nodes() > 2000:
            layout = "sfdp"
        elif self.G.number_of_nodes() > 400:
            r = random.random()
            if r < .4:
                layout = "dot"
            elif r < .6:
                layout="twopi"
            elif r < .8:
                layout="neato"
            else:
                layout = "sfdp"
        elif self.G.number_of_nodes() > 150:
            r = random.random()
            if r < .25:
                layout = "dot"
            elif r < .5:
                layout="twopi"
            elif r < .75:
                layout="neato"
            else:
                layout = "sfdp"
        else:
            r = random.random()
            if r < .33:
                layout = "twopi"
            elif r < .66:
                layout="sfdp"
            else:
                layout="neato"


        def layout_graph(graph):
            if layout == 'twopi':
                pos=nx.graphviz_layout(graph,prog=layout, root=self.blog_name)
            else:
                pos=nx.graphviz_layout(graph,prog=layout)
            return pos

            print "Using layout ", layout

        pos = layout_graph(self.G)

        if random.random() > 0.5 and layout != "sfdp" and layout != "fdp" and layout != "neato":
            new_layout_for_each_frame = True
            print "Using new layout for each frame"
        else:
            new_layout_for_each_frame = False
            print "Using one layout for entire GIF"

        for i,edge_list in enumerate(self.edge_sequence):

            subgraph = nx.DiGraph(edge_list)
            
            if new_layout_for_each_frame:
                
                graph_to_draw = subgraph
                pos=layout_graph(graph_to_draw)

                centrality = nx.closeness_centrality(graph_to_draw,normalized=True)
                centrality_class = pd.Series(centrality)
                threshold = 0.0001
                while threshold <= 1:
                    try:
                        centrality_class[centrality_class<threshold] = 1
                        cat = pd.qcut(centrality_class[centrality_class!=1],
                                      5,labels=[2,3,4,5,6])
                        centrality_class[centrality_class!=1] = pd.np.array(cat)
                        break
                    except Exception, e:
                        threshold += 0.0001
                        continue
                c_class = centrality_class.to_dict()
                node_colors = [c_class[n] for n in graph_to_draw]
                edge_colors = [colorific.norm_color(self.sorted_photo_pallete[6].value) for edge in graph_to_draw.edges(data=False)]
                self.node_cmap = colors.ListedColormap([colorific.norm_color(self.sorted_photo_pallete[5].value),
                                      colorific.norm_color(self.sorted_photo_pallete[4].value),
                                      colorific.norm_color(self.sorted_photo_pallete[3].value),
                                      colorific.norm_color(self.sorted_photo_pallete[2].value),
                                      colorific.norm_color(self.sorted_photo_pallete[1].value),
                                      colorific.norm_color(self.sorted_photo_pallete[0].value)])
            else:

                graph_to_draw = self.G
                node_colors = [self.centrality_class[n] if n in subgraph.nodes() else 0 for n in graph_to_draw]
                edge_colors = [1 if edge in subgraph.edges() else 0 for edge in graph_to_draw.edges(data=False)]
            
            import matplotlib.pyplot as plt

            if layout == "dot":
                fig = plt.figure(1,figsize= (10,min(max([10,self.G.number_of_nodes() / 30]),15)))
            else:
                fig = plt.figure(1,figsize=(10,10))
            

            axes = fig.add_subplot(111)

            # cf = axes.get_figure()
            # cf.set_facecolor('w')

            # plt.hold(True)
            
            
            # draw nodes, coloring by rtt ping time
            nx.draw(graph_to_draw,pos=pos,ax=axes,hold=True,
                    edge_color=edge_colors,
                    node_color=node_colors,
                    cmap=self.node_cmap,
                    edge_cmap=None if new_layout_for_each_frame else self.edge_cmap,
                    with_labels=False,
                    alpha=.8 if layout == 'sfdp' else 1,
                    arrows=False,
                    node_shape='.', #'.' if layout == 'sfdp ' else 'o',
                    node_size= 70000 /graph_to_draw.number_of_nodes(),
                    font_size=24,
                    linewidths=0)

            # node_collection = nx.draw_networkx_nodes(graph_to_draw,pos=pos,ax=axes,
            #         node_color=node_colors,
            #         cmap=self.node_cmap,
            #         alpha=.3 if layout == 'sfdp' else 1,
            #         node_shape='.', #'.' if layout == 'sfdp ' else 'o',
            #         node_size=50000/graph_to_draw.number_of_nodes(),
            #         edgesize=0)

            # edge_collection = nx.draw_networkx_edges(graph_to_draw,pos=pos,ax=axes,
            #         edge_color=edge_colors,
            #         edge_cmap=None if new_layout_for_each_frame else self.edge_cmap,
            #         alpha=.3 if layout == 'sfdp' else 1,
            #         arrows=False)

            # plt.draw_if_interactive()


            # adjust the plot limits
            xmax=1.02*max(xx for xx,yy in pos.values())
            ymax=1.02*max(yy for xx,yy in pos.values())
            plt.xlim(0,xmax)
            plt.ylim(0,ymax)
            
            #add text
            axes.text(0,0,
                      self.edge_sequence_dates[i].strftime('%Y-%m-%d %H:%M'),
                      fontsize=14,
                      color=colorific.norm_color(self.sorted_photo_pallete[6].value))
                        
            fig.savefig('images/frame'+str(i)+'.png', dpi=400/min(max([10,self.G.number_of_nodes() / 30]),15), facecolor = self.background_color)
            plt.clf()
            plt.close()
    
    def write_frames_to_gif(self):
        import os

        # Only store a maximum of 256 GIFs on production server 
        if os.getenv('HOME') == '/home/ubuntu':
            key = "%x" % random.getrandbits(8)
        else:
            key = "%x" % random.getrandbits(32)

        frame_order = [len(self.edge_sequence)-1] + range(len(self.edge_sequence)-1)

        writeGif('images/animated'+key+'.gif',
                 [Image.open('images/frame'+str(i)+'.png').convert(mode="RGB",palette=Image.ADAPTIVE) for i in frame_order],
                 duration=0.15,
                 dither=True)
        self.prev_filename = 'animated'+key+'.gif'
        print "saving as animated"+key+".gif"
        print "\n"

class post_generator(object):
    """"""
    def __init__(self):

        # Auth credentials are stored in secrets.json
        secrets_file = open('secrets.json','rb')
        secrets = json.load(secrets_file)
        secrets_file.close()

        # Build an Authorized Tumblr Client
        self.tumblr_client = pytumblr.TumblrRestClient(**secrets['tumblr_tokens'])

        # Build an Authorized Database Connection
        self.mysql_connection = mysql.connector.connect(**secrets['mysql'])
        
        self.gif_generator = gif_generator()
        
        url = 'http://cl.ly/text/3V32120p461l/A-Thousand-Plateaus.txt'
        text = urllib.urlopen(url).read()
        
        # url = 'http://f.cl.ly/items/3q3r2m0K1G3a2X1A2V1t/anti-oedipus-fixed.txt'
        # text = text + urllib.urlopen(url).read()
        
        # Create an instance of the markov chain. By default, it uses MarkovChain.py's location to
        # store and load its database files to. You probably want to give it another location, like so:
        self.mc = MarkovChain("./markov")
        # To generate the markov chain's language model, in case it's not present
        self.mc.generateDatabase(text)

        try:
            # attempt to keep database size < 2.5GB
            sql = """
            SELECT sum( data_length )/ 1024 / 1024 FROM information_schema.TABLES WHERE table_schema = 'wdmpg' 
            """
            curs = self.mysql_connection.cursor()
            curs.execute(sql)
            size_mb = curs.fetchall()[0][0]
            curs.close()

            if size_mb > 2500:
                sql = """
                delete from tb_posts
                where note_count < 50 and reblogged_root_url is NULL and notes_last_inspected is Null and `reblogs_last_crawled` is Null
                limit 50000
                """
                curs = self.mysql_connection.cursor()
                curs.execute(sql)
                curs.close()

                sql = """
                delete from tb_notes
                where type = 'LIKE'
                limit 100000
                """
                curs = self.mysql_connection.cursor()
                curs.execute(sql)
                curs.close()
            else:
                pass
        except Exception, e:
            pass
   
        
    def generate_random_title(self, seed):

        def title_case(s):
           exceptions = ['a', 'an', 'the', 'at', 'by', 'for', 'in', 'of', 'on', 'to', 'up', 'and', 'as', 'but', 'it', 'or', 'nor']
           word_list = re.split(' ', s)       #re.split behaves as expected
           final = [word_list[0].capitalize()]
           for word in word_list[1:]:
              final.append(word in exceptions and word or word.capitalize())
           return " ".join(final)
        
        
        def sentiment(text):
            try:
                tb = TextBlob(text)
                return tb.sentiment
            except:
                return None
        
        def score(text):
            s = sentiment(text)
            if s == None:
                return 0
            else:
                return s.subjectivity * s.polarity
        
        raw_candidates = [self.mc.generateStringWithSeed(seed) for i in range(1000)]
        caption_candidates = [c for c in raw_candidates if len(c.split(' ')) < 10 and len(c.split(' ')) > 4]
        
        data = [(c, score(c)) for c in caption_candidates]
        sorted_data = sorted(data, key=lambda tup: tup[1], reverse=True)[0:3]
        caption = [s[0] for s in sorted_data][-1]
        
        return title_case(' '.join(word.strip(string.punctuation) for word in caption.split()))

    def edit_submission(self,submission_id,post_url):
        """
        Supply user-submitted links with a GIF and a caption
        """
        blog_name = post_url.split('/')[2].split('.')[0]
        post_id = post_url.split('/post/')[1].split('/')[0]

        # Retrieve Submission from API
        response = self.tumblr_client.posts('wheredidmypostgo', id = submission_id)

        if 'meta' in response.keys():

            if response['meta']['status'] == '404':
                print "No such submission.  Assuming response generated and updating wdmpg_submissions and wdmpg_targets."
                print "    submission_id = ", submission_id
                print "    post_url = ", post_url

                sql = """
                UPDATE wdmpg_submissions 
                SET response_generated = 1 
                WHERE id = %s
                """ % submission_id
                curs = self.mysql_connection.cursor()
                curs.execute(sql)
                curs.close()

                if '.tumblr.com/post/' in post_url:
                    sql = """
                    DELETE FROM wdmpg_targets 
                    WHERE TYPE = 'POST'
                    AND value = %s
                    """ % s['url'].split('/')[4]
                    curs = self.mysql_connection.cursor()
                    curs.execute(sql)
                    curs.close()
                return
            else:
                print "Tumblr API returned: ", response
                return
        else:
            submission_post = response['posts'][0]
        
        self.gif_generator.extract_reblog_graph(post_url)
        
        while True:
            try:
                self.gif_generator.pick_colors()
                break
            except:
                curs = self.mysql_connection.cursor()
                sql = "select name from tb_blogs order by rand() limit 1"
                curs.execute(sql)
                blog_name = curs.fetchall()[0][0]
                curs.close()
                try:
                    self.gif_generator.pick_colors(blog_name=blog_name)
                    break
                except:
                    pass
                
                
        self.gif_generator.draw_graph_frames()
        self.gif_generator.write_frames_to_gif()
        
        # Make Sure GIF is < 1MB
        gif_stats = os.stat('images/'+self.gif_generator.prev_filename)
    
        attempts = 0
        
        while gif_stats.st_size > 1048576 and attempts < 8:
            print "GIF too large, trying again . . ."
            attempts += 1
            while True:
                self.gif_generator.extract_reblog_graph(post_url, frame_rate_multiplier = 1 - float(attempts)/16)
                try:
                    self.gif_generator.pick_colors()
                    break
                except:
                    curs = self.mysql_connection.cursor()
                    sql = "select name from tb_blogs order by rand() limit 1"
                    curs.execute(sql)
                    blog_name = curs.fetchall()[0][0]
                    curs.close()
                    self.gif_generator.pick_colors(blog_name=blog_name)
                    break
            self.gif_generator.draw_graph_frames()
            self.gif_generator.write_frames_to_gif()

            gif_stats = os.stat('images/'+self.gif_generator.prev_filename)
            
            if attempts > 8:
                raise BaseException("more than 8 attempts")
        
        print "GIF is ready and under 1MB"

        # handle redirects
        opener = urllib2.build_opener(urllib2.HTTPRedirectHandler)
        request = opener.open(post_url)
        post_url = request.url

        post_id = post_url.split('/post/')[1].split('/')[0]
        
        sql = "select id, blog_name, short_url, note_count from tb_posts where post_url = %s"
        curs = self.mysql_connection.cursor()
        curs.execute(sql,(post_id,))
        post_id, blog_name, short_url, note_count = curs.fetchall()[0]
        curs.close()
        
        n_nodes = self.gif_generator.G.number_of_nodes()
         
        if submission_post['title'] != 'Submitted Post':
            try:
                header_html = '<br><h3>' + submission_post['title'] + '</h3>'
            except:
                header_html = '<br>'
            header_html = header_html +' ' + submission_post['description'] + '<br>'
        else:
            header_html = '<br>'+submission_post['description'].split('<h4>Reblog network of an <a href="')[0]
            
            
        header_html = header_html+'<h4>Reblog network of an <a href="'+short_url+'">original tumblr post</a> by <a href="'+ post_url.split('/post')[0]+'">'+blog_name+'</a></h4>'
        caption_html = '<br><p><p></p>This image is based on '+str(n_nodes)+' reblogs ('+ str(round(float(n_nodes)/note_count*100,5))+'% of the total number of notes on the post).  '
        if float(n_nodes)/note_count < .1:
            caption_html = caption_html + 'The bot that runs <a href="http://wheredidmypostgo.tumblr.com">Where Did my Post Go?</a> samples reblog graphs from currently active "leaves" back to the original "root" blog; so keep in mind that while this may not be the complete network, it is currently the most active "branch".</p>'
        else:
            pass
        caption_html = caption_html + '<br><h4>The most influential nodes in this reblog graph are </h4>'
        
        blog = self.tumblr_client.posts(blog_name, id = post_id)['blog']
        post = self.tumblr_client.posts(blog_name, id = post_id)['posts'][0]

        slug_seed = self.generate_title_seed(blog_name, post_id)
        slug = "-".join([s.lower() for s in self.generate_random_title(slug_seed).split(" ")])
        print "slug = "+slug



        import networkx as nx
        import pandas.io.sql as psql
        
        cent = nx.closeness_centrality(self.gif_generator.G,normalized=True)
        top_five_influencers = sorted(cent.iterkeys(), key=lambda x: cent[x], reverse=True)[0:5]
        
        sql = """
        select short_url as "Post", blog_name as "Blog Name"
        from tb_posts
        where blog_name in (%s,%s,%s,%s,%s)
            and reblogged_root_url = %s
        """
        
        influencer_df = psql.read_frame(sql,self.mysql_connection,
                                        index_col="Blog Name",
                                        params = top_five_influencers + [post_url])
        influencer_df['Betweenness Centrality'] = 0
        for i in influencer_df.index:
            influencer_df['Betweenness Centrality'].ix[i] = cent[i]
        
        
        def formatter(influencer_post,betweenness_centrality):
            return ' (<a href="'+influencer_post+'">reblog</a>, '+str(round(betweenness_centrality,3))+' <a href="http://en.wikipedia.org/wiki/Betweenness_centrality">Betweenness Centrality</a>)'
        
        influencer_html = '<ul>'
        for influencer_blog,(influencer_post,betweenness_centrality) in influencer_df.sort(columns=['Betweenness Centrality'],ascending=False).iterrows():
            influencer_html = influencer_html+'<li>'
            influencer_html = influencer_html + influencer_blog + formatter(influencer_post,betweenness_centrality)
            influencer_html = influencer_html+'</li>'
        influencer_html = influencer_html+'</ul>' 
        
        response = self.tumblr_client.create_photo('wheredidmypostgo',
                                        data='images/'+self.gif_generator.prev_filename,
                                        state='draft')
        draft_id = response['id']
        
        draft_post = self.tumblr_client.posts('wheredidmypostgo',id=draft_id)['posts'][0]
        
        print "created draft post to host image ", draft_post['post_url']

        photo_html = '<img src="'+draft_post['photos'][0]['original_size']['url']+'" alt="Reblog network of an original tumblr post by '+blog_name+'">'
        
        description_html = header_html+photo_html+caption_html+influencer_html


        
        blogname = 'wheredidmypostgo'
        
        tags = ['gif','data','network','graph','tumblr','data visualization','trees','animation','animated','color','artists on tumblr']
        pd.np.random.shuffle(tags)
        
        tags = ['wheredidmypostgo'] + tags
        
        kwargs = {'tags': tags,
                  'id': submission_id,
                  'state': 'submission',
                  'description': description_html.encode('ascii','ignore'),
                  'format': 'html',
                  'title': 'Submitted Post',
                  'slug': slug}
        
        url = "/v2/blog/%s/post/edit" % blogname
        
        if 'tags' in kwargs and kwargs['tags']:
            # Take a list of tags and make them acceptable for upload
            kwargs['tags'] = ",".join(kwargs['tags'])
        
        valid_options = ['id'] + ['type', 'state', 'tags', 'tweet', 'date', 'format', 'slug'] + ['title', 'url', 'description']
        
        self.tumblr_client.send_api_request('post', url, kwargs, valid_options) 

        print "edited submission http://www.tumblr.com/edit/"+str(submission_id)

    def edit_all_submissions(self):
        sql = """
        select id, url
        from wdmpg_submissions
        where response_generated = 0
        order by RAND()
        """
        curs = self.mysql_connection.cursor()
        curs.execute(sql)
        submissions = curs.fetchall()
        curs.close()
        for submission_id,post_url in submissions:
            try:
                self.edit_submission(submission_id,post_url)
            except Exception, e:
                import traceback
                traceback.print_exc()
                pass


    def determine_photo_post_type(self, post_url):
        
        try:
            sql = """
            select id, blog_name, featured_in_tag
            from tb_posts
            where post_url = %s
            limit 1
            """ 
            curs = self.mysql_connection.cursor()
            curs.execute(sql,(post_url,))
            post_id, blog_name, featured_in_tag = curs.fetchall()[0]
            print "featured_in_tag = " + str(featured_in_tag)
            curs.close()
        except:
            opener = urllib2.build_opener(urllib2.HTTPRedirectHandler)
            request = opener.open(post_url)
            post_url = request.url
            
            sql = """
            select id, blog_name, featured_in_tag
            from tb_posts
            where post_url = %s
            limit 1
            """ 
            curs = self.mysql_connection.cursor()
            curs.execute(sql,(post_url,))
            post_id, blog_name, featured_in_tag = curs.fetchall()[0]
            print "featured_in_tag = " + str(featured_in_tag)
            curs.close()
        
        sql = """
        select count(value)
        from wdmpg_targets
        where blog_name = %s
        """ 
        curs = self.mysql_connection.cursor()
        curs.execute(sql,(blog_name,))
        target = curs.fetchall()[0][0]
        print "target = ", target
        if target > 0:
            anon = True
        else:
            anon = False
        curs.close()
        
        if blog_name == 'wheredidmypostgo':
            return 'meta'
        elif anon:
            return 'anonymous'
        elif featured_in_tag != None:
            return 'featured'
        else: 
            return 'named'
        
               
    def generate_photo_posts(self, count = 5):
        sql = """
        select reblogged_root_url as url
        from (
        select reblogged_root_url, 
        count(id) as count, 
        POW( ((count(id)/max(note_count)) / 0.5), 0.5) * POW((count(id)/1000),0.5) as graph_score, 
        POW(1/TIMESTAMPDIFF(DAY,min(date),now()),.5) as recency_score 
        from tb_posts
        where reblogged_root_url is not null
            and date > DATE_SUB(NOW(), INTERVAL 14 day)
        group by reblogged_root_url
        order by POW(POW(((count(id)/max(note_count)) / 0.5), 0.5) * POW((count(id)/1000),0.5),0.5)*POW(POW(1/TIMESTAMPDIFF(DAY,min(date),now()),.5),0.5) DESC
        limit 300
        ) as top_graphs
        where reblogged_root_url not in (select url from wdmpg_submissions)
        order by rand() limit %s
        """ % count
        curs = self.mysql_connection.cursor()
        curs.execute(sql)
        
        for url in curs.fetchall():
            reblogged_root_url = url[0]
            try:
                print "reblogged_root_url = " + reblogged_root_url
                post_type = self.determine_photo_post_type(reblogged_root_url)
                print "post_type = " + post_type
                print "\n"
                if post_type == 'anonymous':
                    self.generate_anonymous_photo_post(reblogged_root_url)
                if post_type == 'named':
                    self.generate_named_photo_post(reblogged_root_url)
                if post_type == 'featured':
                    self.generate_featured_photo_post(reblogged_root_url)
                if post_type == 'meta':
                    self.generate_meta_photo_post(reblogged_root_url)
            except Exception, e:
                import traceback
                traceback.print_exc()
                pass
            
        curs.close()
    
    
    def generate_title_seed(self, blog_name, post_id):
        
        post = self.tumblr_client.posts(blog_name, id=post_id)['posts'][0]
        
        if len(post['slug']) > 0:
            return str(" ".join(post['slug'].split('-')[:2]))
        elif len(post['tags']) > 0:
            return str(sorted(post['tags'], key=lambda s: len([c for c in s if c ==' ']), reverse = True)[0])
        else:
            try:
                return str(" ".join(post['title'].split(' ')[:2]))
            except:
                return "reblog network"
    
    def generate_anonymous_photo_post(self, reblogged_root_url):
        sql = """
        select id, blog_name, note_count
        from tb_posts
        where post_url = %s
        limit 1
        """ 
        curs = self.mysql_connection.cursor()
        curs.execute(sql,(reblogged_root_url,))
        post_id, blog_name, note_count = curs.fetchall()[0]
        curs.close()
        
        ######
        
        self.gif_generator.extract_reblog_graph(reblogged_root_url)
        
        while True:
            try:
                self.gif_generator.pick_colors()
                break
            except:
                curs = self.mysql_connection.cursor()
                sql = "select name from tb_blogs order by rand() limit 1"
                curs.execute(sql)
                blog_name = curs.fetchall()[0][0]
                curs.close()
                self.gif_generator.pick_colors(blog_name=blog_name)
                break
                
        self.gif_generator.draw_graph_frames()
        self.gif_generator.write_frames_to_gif()
        
        # Make Sure GIF is < 1MB
        gif_stats = os.stat('images/'+self.gif_generator.prev_filename)
    
        attempts = 0
        
        while gif_stats.st_size > 1048576 and attempts < 8:
            print "GIF too large, trying again . . ."
            attempts += 1
            while True:
                self.gif_generator.extract_reblog_graph(reblogged_root_url, frame_rate_multiplier = 1 - float(attempts)/16)
                try:
                    self.gif_generator.pick_colors()
                    break
                except:
                    curs = self.mysql_connection.cursor()
                    sql = "select name from tb_blogs order by rand() limit 1"
                    curs.execute(sql)
                    blog_name = curs.fetchall()[0][0]
                    curs.close()
                    self.gif_generator.pick_colors(blog_name)
                    break
            self.gif_generator.draw_graph_frames()
            self.gif_generator.write_frames_to_gif()

            gif_stats = os.stat('images/'+self.gif_generator.prev_filename)
            
            if attempts > 8:
                raise BaseException("more than 8 attempts")
        
        print "GIF is ready and under 1MB"
        
        slug_seed = "reblog network"
        slug = "-".join([s.lower() for s in self.generate_random_title(slug_seed).split(" ")])
        print "slug = "+slug
        
        post = self.tumblr_client.posts(blog_name, id = post_id)['posts'][0]
        
        n_nodes = self.gif_generator.G.number_of_nodes()
        
        caption_html = '<h4> Reblog network of an anonymous tumblr post. </h4>'
        caption_html = caption_html + '<br><p><p></p>This image is based on '+str(n_nodes)+' reblogs ('+ str(round(float(n_nodes)/post['note_count']*100,5))+'% of the total number of notes on the post).  '
        caption_html = caption_html +'<br><br><p><a href="http://wheredidmypostgo.tumblr.com">Where Did my Post Go?</a> is a bot that posts GIFs generated from the reblog networks of posts that you submit '
        
        caption_html = caption_html + """
                                      (more info: 
                                      <a href='http://wheredidmypostgo.tumblr.com'>tumblr</a>, 
                                      <a href='https://github.com/nmacri/where-did-my-post-go'>github</a>, 
                                      <a href='http://www.reddit.com/r/dataisbeautiful/comments/22dkm3/reblog_network_of_an_anonymous_tumblr_post_oc/'>reddit</a>.)</p>
                                      """
        tags = ['gif','data','network','graph','graphs','tumblr','data visualization','animation','animated','color','design','code','art','digital art','generative','datamosh','glitch','abstract','glitch art','artists on tumblr']
        pd.np.random.shuffle(tags)
        
        tags = ['wheredidmypostgo'] + tags
        
        response = self.tumblr_client.create_photo('wheredidmypostgo',
                                        data='images/'+self.gif_generator.prev_filename,
                                        tags = tags,
                                        caption = caption_html,
                                        slug = slug,
                                        state='queue')
        print response
        
        return response
    
    def generate_named_photo_post(self, reblogged_root_url):
        sql = """
        select id, blog_name, note_count
        from tb_posts
        where post_url = %s
        limit 1
        """ 
        curs = self.mysql_connection.cursor()
        curs.execute(sql,(reblogged_root_url,))
        post_id, blog_name, note_count = curs.fetchall()[0]
        curs.close()
        
        ######
        
        self.gif_generator.extract_reblog_graph(reblogged_root_url)
        
        while True:
            try:
                self.gif_generator.pick_colors()
                break
            except:
                curs = self.mysql_connection.cursor()
                sql = "select name from tb_blogs order by rand() limit 1"
                curs.execute(sql)
                blog_name = curs.fetchall()[0][0]
                curs.close()
                self.gif_generator.pick_colors(blog_name=blog_name)
                break
                
        self.gif_generator.draw_graph_frames()
        self.gif_generator.write_frames_to_gif()
        
        # Make Sure GIF is < 1MB
        gif_stats = os.stat('images/'+self.gif_generator.prev_filename)
    
        attempts = 0
        
        while gif_stats.st_size > 1048576 and attempts < 8:
            print "GIF too large, trying again . . ."
            attempts += 1
            while True:
                self.gif_generator.extract_reblog_graph(reblogged_root_url, frame_rate_multiplier = 1 - float(attempts)/16)
                try:
                    self.gif_generator.pick_colors()
                    break
                except:
                    curs = self.mysql_connection.cursor()
                    sql = "select name from tb_blogs order by rand() limit 1"
                    curs.execute(sql)
                    blog_name = curs.fetchall()[0][0]
                    curs.close()
                    self.gif_generator.pick_colors(blog_name=blog_name)
                    break
            self.gif_generator.draw_graph_frames()
            self.gif_generator.write_frames_to_gif()

            gif_stats = os.stat('images/'+self.gif_generator.prev_filename)
            
            if attempts > 8:
                raise BaseException("more than 8 attempts")
        
        print "GIF is ready and under 1MB"
        
        blog = self.tumblr_client.posts(blog_name, id = post_id)['blog']
        post = self.tumblr_client.posts(blog_name, id = post_id)['posts'][0]
        
        n_nodes = self.gif_generator.G.number_of_nodes()
        
        caption_html = '<h4> Reblog network of an <a href="'+post['post_url']+'">original tumblr post</a> by <a href="'+blog['url']+'">'+blog['name']+'</a></h4>'
        caption_html = caption_html + '<br><p><p></p>This image is based on '+str(n_nodes)+' reblogs ('+ str(round(float(n_nodes)/post['note_count']*100,5))+'% of the total number of notes on the post).  '
        caption_html = caption_html +'<br><br><p><a href="http://wheredidmypostgo.tumblr.com">Where Did my Post Go?</a> is a bot that posts GIFs generated from the reblog networks of posts that you submit '
        
        caption_html = caption_html + """
                                      (more info: 
                                      <a href='http://wheredidmypostgo.tumblr.com'>tumblr</a>, 
                                      <a href='https://github.com/nmacri/where-did-my-post-go'>github</a>, 
                                      <a href='http://www.reddit.com/r/dataisbeautiful/comments/22dkm3/reblog_network_of_an_anonymous_tumblr_post_oc/'>reddit</a>).</p>
                                      """
        
        slug_seed = self.generate_title_seed(blog_name, post_id)
        slug = "-".join([s.lower() for s in self.generate_random_title(slug_seed).split(" ")])
        print "slug = "+slug
        
        tags = ['gif','data','network','graph','graphs','tumblr','data visualization','animation','animated','color','design','code','art','digital art','generative','datamosh','glitch','abstract','glitch art','artists on tumblr']
        pd.np.random.shuffle(tags)
        
        tags = ['wheredidmypostgo'] + tags
        
        response = self.tumblr_client.create_photo('wheredidmypostgo',
                                        data='images/'+self.gif_generator.prev_filename,
                                        tags = tags,
                                        caption = caption_html,
                                        slug = slug,
                                        state='queue')
        print response
        
        return response
    
    def generate_featured_photo_post(self, reblogged_root_url):
        sql = """
        select id, blog_name, note_count, featured_in_tag
        from tb_posts
        where post_url = %s
        limit 1
        """ 
        curs = self.mysql_connection.cursor()
        curs.execute(sql,(reblogged_root_url,))
        post_id, blog_name, note_count, featured_in_tag = curs.fetchall()[0]
        curs.close()
        
        ######
        
        self.gif_generator.extract_reblog_graph(reblogged_root_url)
        
        while True:
            try:
                self.gif_generator.pick_colors()
                break
            except:
                curs = self.mysql_connection.cursor()
                sql = "select name from tb_blogs order by rand() limit 1"
                curs.execute(sql)
                blog_name = curs.fetchall()[0][0]
                curs.close()
                self.gif_generator.pick_colors(blog_name=blog_name)
                break
                
        self.gif_generator.draw_graph_frames()
        self.gif_generator.write_frames_to_gif()
        
        # Make Sure GIF is < 1MB
        gif_stats = os.stat('images/'+self.gif_generator.prev_filename)
    
        attempts = 0
        
        while gif_stats.st_size > 1048576 and attempts < 8:
            print "GIF too large, trying again . . ."
            attempts += 1
            while True:
                self.gif_generator.extract_reblog_graph(reblogged_root_url, frame_rate_multiplier = 1 - float(attempts)/16)
                try:
                    self.gif_generator.pick_colors()
                    break
                except:
                    curs = self.mysql_connection.cursor()
                    sql = "select name from tb_blogs order by rand() limit 1"
                    curs.execute(sql)
                    blog_name = curs.fetchall()[0][0]
                    curs.close()
                    self.gif_generator.pick_colors(blog_name=blog_name)
                    break
            self.gif_generator.draw_graph_frames()
            self.gif_generator.write_frames_to_gif()

            gif_stats = os.stat('images/'+self.gif_generator.prev_filename)
            
            if attempts > 8:
                raise BaseException("more than 8 attempts")
        
        print "GIF is ready and under 1MB"
        
        blog = self.tumblr_client.posts(blog_name, id = post_id)['blog']
        post = self.tumblr_client.posts(blog_name, id = post_id)['posts'][0]
        
        n_nodes = self.gif_generator.G.number_of_nodes()
        
        caption_html = '<h4> Reblog network of a <a href="'+post['post_url']+'">featured tumblr post</a> by <a href="'+blog['url']+'">'+blog['name']+'</a> (featured in: '+featured_in_tag+')</h4>'
        caption_html = caption_html + '<br><p><p></p>This image is based on '+str(n_nodes)+' reblogs ('+ str(round(float(n_nodes)/post['note_count']*100,5))+'% of the total number of notes on the post).  '
        caption_html = caption_html +'<br><br><p><a href="http://wheredidmypostgo.tumblr.com">Where Did my Post Go?</a> is a bot that posts GIFs generated from the reblog networks of posts that you submit '
        
        caption_html = caption_html + """
                                      (more info: 
                                      <a href='http://wheredidmypostgo.tumblr.com'>tumblr</a>, 
                                      <a href='https://github.com/nmacri/where-did-my-post-go'>github</a>, 
                                      <a href='http://www.reddit.com/r/dataisbeautiful/comments/22dkm3/reblog_network_of_an_anonymous_tumblr_post_oc/'>reddit</a>).</p>
                                      """
        
        slug_seed = self.generate_title_seed(blog_name, post_id)
        slug = "-".join([s.lower() for s in self.generate_random_title(slug_seed).split(" ")])
        print "slug = "+slug
        
        tags = ['gif','data','network','graph','tumblr','data visualization','animation','animated','color','design','code','art','digital art','generative','datamosh','glitch','abstract','glitch art']
        pd.np.random.shuffle(tags)
        
        tags = ['wheredidmypostgo'] + tags
        
        
        response = self.tumblr_client.create_photo('wheredidmypostgo',
                                        data='images/'+self.gif_generator.prev_filename,
                                        tags = tags,
                                        caption = caption_html,
                                        slug = slug,
                                        state='queue')
        print response
        
        return response
    
    def generate_meta_photo_post(self, reblogged_root_url):
        sql = """
        select id, blog_name, note_count
        from tb_posts
        where post_url = %s
        limit 1
        """ 
        curs = self.mysql_connection.cursor()
        curs.execute(sql,(reblogged_root_url,))
        post_id, blog_name, note_count = curs.fetchall()[0]
        curs.close()
        
        ######
        
        self.gif_generator.extract_reblog_graph(reblogged_root_url)
        
        while True:
            try:
                curs = self.mysql_connection.cursor()
                sql = "select name from tb_blogs order by rand() limit 1"
                curs.execute(sql)
                blog_name = curs.fetchall()[0][0]
                curs.close()
                self.gif_generator.pick_colors(blog_name=blog_name)
                break
            except:
                curs = self.mysql_connection.cursor()
                sql = "select name from tb_blogs order by rand() limit 1"
                curs.execute(sql)
                blog_name = curs.fetchall()[0][0]
                curs.close()
                self.gif_generator.pick_colors(blog_name=blog_name)
                break
                
        self.gif_generator.draw_graph_frames()
        self.gif_generator.write_frames_to_gif()
        
        # Make Sure GIF is < 1MB
        gif_stats = os.stat('images/'+self.gif_generator.prev_filename)
    
        attempts = 0
        
        while gif_stats.st_size > 1048576 and attempts < 8:
            print "GIF too large, trying again . . ."
            attempts += 1
            while True:
                self.gif_generator.extract_reblog_graph(reblogged_root_url, frame_rate_multiplier = 1 - float(attempts)/16)
                try:
                    curs = self.mysql_connection.cursor()
                    sql = "select name from tb_blogs order by rand() limit 1"
                    curs.execute(sql)
                    blog_name = curs.fetchall()[0][0]
                    curs.close()
                    self.gif_generator.pick_colors(blog_name=blog_name)
                    break
                except:
                    curs = self.mysql_connection.cursor()
                    sql = "select name from tb_blogs order by rand() limit 1"
                    curs.execute(sql)
                    blog_name = curs.fetchall()[0][0]
                    curs.close()
                    self.gif_generator.pick_colors(blog_name=blog_name)
                    break
            self.gif_generator.draw_graph_frames()
            self.gif_generator.write_frames_to_gif()

            gif_stats = os.stat('images/'+self.gif_generator.prev_filename)
            
            if attempts > 8:
                raise BaseException("more than 8 attempts")
        
        print "GIF is ready and under 1MB"
        
        blog = self.tumblr_client.posts(blog_name, id = post_id)['blog']
        post = self.tumblr_client.posts(blog_name, id = post_id)['posts'][0]
        
        n_nodes = self.gif_generator.G.number_of_nodes()
        
        caption_html = caption_html + '<h4> Meta Post</h4> <br><br><p> Reblog network of an <a href="'+post['post_url']+'">original tumblr post</a> by <a href="'+blog['url']+'">'+blog['name']+'</a></p>'
        caption_html = caption_html + '<br><p><p></p>This image is based on '+str(n_nodes)+' reblogs ('+ str(round(float(n_nodes)/post['note_count']*100,5))+'% of the total number of notes on the post).  '
        caption_html = caption_html +'<br><br><p><a href="http://wheredidmypostgo.tumblr.com">Where Did my Post Go?</a> is a bot that posts GIFs generated from the reblog networks of posts that you submit '
        
        caption_html = caption_html + """
                                      (more info: 
                                      <a href='http://wheredidmypostgo.tumblr.com'>tumblr</a>, 
                                      <a href='https://github.com/nmacri/where-did-my-post-go'>github</a>, 
                                      <a href='http://www.reddit.com/r/dataisbeautiful/comments/22dkm3/reblog_network_of_an_anonymous_tumblr_post_oc/'>reddit</a>).</p>
                                      """     
        
        
        tags = ['gif','data','network','graph','tumblr','data visualization','animation','animated','color','design','code','art','digital art','generative','datamosh','glitch','abstract','glitch art']
        pd.np.random.shuffle(tags)
        
        tags = ['meta post','wheredidmypostgo'] + tags
        
        response = self.tumblr_client.create_photo('wheredidmypostgo',
                                        data='images/'+self.gif_generator.prev_filename,
                                        tags = tags,
                                        caption = caption_html,
                                        slug = 'reblog-network-meta-post',
                                        state='queue')
        print response
        
        return response



    def publish_best_submission(self):

        def __generate_submissions(self):
            offset = 0
            batch =  self.tumblr_client.submission('wheredidmypostgo', offset = offset)['posts']
            yield batch
            while len(batch) == 10:
                offset += 10
                batch =  self.tumblr_client.submission('wheredidmypostgo', offset = offset)['posts']
                yield batch

        generator = __generate_submissions(self)
        submissions = []
        
        for p in generator:
            submissions.extend(p)
        
        def score(s):
            v =  s['description'].split('This image is based on ')[1].split('% of the total number of notes')[0].split(' reblogs (')
            nodes, pct = (int(v[0]),float(v[1]))
            score = (float(nodes)/8000)**.5 * (pct/40)**.5
            return score
            
            
        def is_edited(s):
            return 'description' in s.keys() and '<a href="http://en.wikipedia.org/wiki/Betweenness_centrality">' in s['description']
            
        
        submission_scores = [{'submission_id':s['id'], 'score': score(s)} for s in submissions if is_edited(s)]
        
        best_submission = [s for s in submissions if s['id'] == sorted(submission_scores, key=lambda s: s['score'], reverse=True)[0]['submission_id']][0]
                
        blog_name = 'wheredidmypostgo'
        url = "/v2/blog/%s/post/edit" % blog_name 
        kwargs = {'id': best_submission['id'],
                  'state':'published'}
        valid_options = ['id', 'type', 'state', 'tags', 'tweet', 'date', 'format', 'slug', 'title', 'url', 'description']

        response = self.tumblr_client.send_api_request('post', url, kwargs, valid_options) 


        sql = """
        UPDATE wdmpg_submissions 
        SET response_generated = 1 
        WHERE id = %s
        """ % best_submission['id']
        curs = self.mysql_connection.cursor()
        curs.execute(sql)
        curs.close()

        if '.tumblr.com/post/' in best_submission['url']:
            sql = """
            DELETE FROM wdmpg_targets 
            WHERE TYPE = 'POST'
            AND value = %s
            """ % s['url'].split('/')[4]
            curs = self.mysql_connection.cursor()
            curs.execute(sql)
            curs.close()

        return response


